<< "main-setup", include = FALSE >>=
## Packages
library(ciCalibrate) # support intervals
library(lamW) # Lambert W function
library(colorspace) # color palettes
library(ggplot2) # plotting

## Function to nicely format Bayes factors
.formatBF_ <- function(BF, digits = "default") {
    ## check inputs
    stopifnot(
        length(BF) == 1,
        is.numeric(BF),
        (is.finite(BF) && 0 < BF) || is.na(BF),

        length(digits) == 1,
        (is.character(digits) && digits == "default") ||
        (is.numeric(digits) && 0 <= digits)
    )
    ## return NA if input NA/NaN
    if (is.na(BF) || is.nan(BF))
        result <- NA
    else {
        ## format BF
        if (digits == "default") {
            if (BF < 1/1000)
                result <- "< 1/1000"
            if ((BF >= 1/1000) & (BF <= 1/10))
                result <- paste0("1/", as.character(round(1/BF)))
            if ((BF > 1/10) & (BF < 1))
                result <- paste0("1/", as.character(round(1/BF, digits = 1)))
            if ((BF < 10) & (BF >= 1))
                result <- as.character(round(BF, digits = 1))
            if ((BF >= 10) & (BF <= 1000))
                result <- as.character(round(BF))
            if (BF > 1000)
                result <- "> 1000"
        } else {
            if (BF < 1)
                result <- paste0("1/", as.character(round(1/BF, digits = digits)))
            else
                result <- as.character(round(BF, digits = digits))
        }
        ## when 1/1 return 1
        if (result == "1/1") result <- "1"
    }
    return(result)
}
formatBF <- Vectorize(FUN = .formatBF_)
@

\section{Introduction}
A pervasive problem in data analysis is to draw inferences about unknown
parameters of statistical models. For instance, data analysts are often
interested in identifying a set of parameter values which are relatively
compatible with the observed data. Here we focus on a particular method for
doing so ---the \emph{support set}--- that arguably represents
a %a natural Bayesian and likelihoodist
natural evidential answer to the problem \citep{Edwards1971, Royall1997}. The
evidential paradigm defines statistical evidence via the \emph{Law of
  Likelihood} \citep{Hacking1965}, that is, data constitute evidence for one
parameter value over an alternative parameter value if the likelihood of the
data under that parameter value is larger than under the alternative parameter
value. The likelihood ratio (or Bayes factor) measures the strength of evidence,
and it plays also a central role in the construction of support sets, as we will
explain in the following.


Let $f(x \given \theta)$ denote the likelihood of the observed data $x$. Let
$\theta$ be an unknown parameter and denote by
\begin{align}
    \BF_{01}(x; \theta_0)
    \label{eq:bf}
    % = \frac{f(x\given\h{0}\colon \theta = \theta_0)}{
    % f(x\given\h{1}\colon \theta \neq \theta_0)}
    = \frac{f(x\given\h{0})}{f(x\given\h{1})}
    = \frac{f(x\given\theta_0)}{\int f(x\given \theta) \,
    f(\theta \given \h{1})\,\text{d}\theta}
\end{align}
the Bayes factor quantifying the strength of evidence which the observed data
$x$ provide for the simple null hypothesis $\h{0} \colon \theta = \theta_0$
relative to a (possibly composite) alternative hypothesis
$\h{1} \colon \theta \neq \theta_0$, with $f(x\given\h{1})$ the marginal
likelihood of $x$ obtained from integrating the likelihood $f(x \given \theta)$
with respect to the prior density of the parameter $f(\theta \given \h{1})$
under the alternative $\h{1}$ \citep{Jeffreys1961, Kass1995}. For constructing a
support interval, one views the Bayes
factor~\eqref{eq:bf} %$\BF_{01}(x; \theta_0)$
as a function of the null value $\theta_0$ for fixed data $x$. A $k$
\emph{support set} for $\theta$ is then given by the set of parameter values for
which the data are $k$ times more likely than under the alternative hypothesis
$\h{1}$ \citep{Wagenmakers2020}, that is,
\begin{align}
    \label{eq:ss}
    \mbox{SI}_k = \big\{\theta_0 : \BF_{01}(x; \theta_0) \geq k \big\}.
\end{align}
The support set thus includes the parameter values for which the observed data
provide statistical evidence of at least level $k$.

\begin{figure*}[!htb]
<< "figure-BFfun-RECOVERY", fig.height = ifelse(type == "journal", 3.1, 3.5) >>=
## data from RECOVERY trial abstract
## https://www.nejm.org/doi/10.1056/NEJMoa2021436
HR <- 0.83
ciHR <- c(0.75, 0.93)
logHR <- log(HR)
selogHR <- diff(log(ciHR))/(2*qnorm(p = 0.975))

## "power to detect a clinically relevant proportional reduction of 20% (an
## absolute difference of 4 percentage points)"
m <- log(0.8)
v <- 4 # unit information variance for a logHR
nevents <- 4/selogHR^2 # implicit event count

## compute k SI for different k
k <- c(1/10, 1, 10)
siList <- lapply(X = k, FUN = function(k) {
    si <- ciCalibrate(estimate = logHR, se = selogHR, siLevel = k,
                      method = "SI-normal", priorMean = m, priorSD = sqrt(v))
})
siDF <- do.call("rbind", lapply(X = siList, FUN = function(x) {
    data.frame(k = x$siLevel, lower = x$si[1], upper = x$si[2],
               est = x$estimate)
}))

## compute BF function
logHRseq <- seq(log(0.65), log(1.05), length.out = 500)
bfFun <- siList[[1]]$bfFun
plotDF <- data.frame(logHR = logHRseq, HR = exp(logHRseq),
                     BF = bfFun(x = logHRseq))

## plot BFfun and k SI
bfBks <- c(1, 3, 10, 30, 100, 300, 1000)
bfBks <- c(1, 10, 100, 1000)
bfBks <- c(1/bfBks, bfBks)
bfMax <- bfFun(x = logHR)
plot <- ggplot(data = plotDF, aes(x = logHR, y = BF)) +
    geom_ribbon(aes(ymin = 0, ymax = BF), alpha = 0.05, fill = 1) +
    geom_line(size = 0.3) +
    annotate(geom = "segment", x = rep(-0.42, 2), xend = rep(-0.42, 2),
             y = 1.1^(c(-1, 1)), yend = 5^(c(-1, 1)),
             arrow = arrow(length = unit(0.07, "inches")),
             lineend = "round", linejoin = "round", alpha = 0.6) +
    annotate(geom = "text", x = rep(-0.4075, 2), y = 2^(c(-1, 1)),
             label = c("italic('H')[1]", "italic('H')[0]"), alpha = 0.6,
             parse = TRUE, size = 4) +
    geom_errorbarh(data = siDF, aes(xmin = lower, xmax = upper, y = k),
                   height = 0.15, size = 0.3) +
    annotate(geom = "point", x = logHR, y = bfMax, size = 0.7) +
        annotate(geom = "text", x = logHR, y = bfMax*1.5, size = 3,
                 label = "'MLE' ~ hat(theta)", parse = TRUE) +
    geom_text(data = siDF,
              aes(x = est, y = k*1.4,
                  label = paste0("italic(k) == ", formatBF(BF = k))),
              parse = TRUE,
              size = ifelse(type == "journal", 3.5, 4)) +
    scale_y_log10(breaks = bfBks, labels = formatBF(BF = bfBks)) +
    coord_cartesian(ylim = c(1/100, 100), xlim = c(-0.41, 0.035)) +
    labs(x = bquote("Log hazard ratio" ~ theta[scriptstyle("0")]),
         y = bquote("BF"["01"] * "(data; " * theta[scriptstyle("0")] * ")")) +
    theme_bw()
if (type == "journal") {
    plot <- plot ## +
        ## theme_bw(base_size = 8)
}
plot +
    theme(panel.grid.minor = element_blank())
@

\caption{Application of support intervals to data from the RECOVERY trial
  \citep{RECOVERY2021}. The trial led to an estimated (age-adjusted) log hazard
  ratio of $\that = \Sexpr{round(logHR, 2)}$ (95\% confidence interval from
  $\Sexpr{round(log(ciHR)[1], 2)}$ to $\Sexpr{round(log(ciHR)[2], 2)}$) for
  Covid-19 mortality in hospitalized patients treated with dexamethasone
  compared to usual care. %standard error $\sigma = 0.05$.
  The Bayes factor for testing $\h{0}\colon \theta = \theta_0$ versus
  $\h{1}\colon \theta \neq \theta_0$ is shown as a function of the null value
  $\theta_0$. A normal distribution centered around the clinically relevant
  proportional mortality reduction $\mu_\theta = \log(0.8) = -0.22$ (as deemed
  by the trial steering committee) with unit variance $\sigma_\theta^{2} = 4$
  \citep[ch. 2.4.2]{Spiegelhalter2004} is used as a prior for $\theta$ under the
  alternative $\h{1}$. Support intervals are shown for different support levels
  $k$.}
    \label{fig:example}
\end{figure*}

Figure~\ref{fig:example} illustrates different support sets (in this case
intervals) for a log hazard ratio parameter $\theta$ quantifying the effect of
the drug dexamethasone on the mortality of hospitalized patients with Covid-19
enrolled in the RECOVERY trial \citep{RECOVERY2021}. Shown is also the Bayes
factor for testing $\h{0}\colon \theta = \theta_0$ versus
$\h{1}\colon \theta \neq \theta_0$ viewed as a function of the null value
$\theta_0$. A $k$ support set is obtained from ``cutting'' this function at
height $k$, and taking the parameter values with a Bayes factor value larger
than $k$ as part of the set. In practice, it is not clear which value of $k$
should be chosen. One possibility is to select $k$ based on conventional
classifications of Bayes factors or likelihood ratios. Table~\ref{tab:evidence}
lists three of them. For instance, using the classification from
\citet{Jeffreys1961}, the $k=10$ support interval ranging from
$\Sexpr{round(siDF[3,2], 2)}$ to $\Sexpr{round(siDF[3,3], 2)}$ can be
interpreted to contain log hazard ratios that are \emph{strongly supported} by
the data, whereas the $k=1/10$ support interval ranging from
$\Sexpr{round(siDF[1,2], 2)}$ to $\Sexpr{round(siDF[1,3], 2)}$ can be
interpreted to contain log hazard ratios that are \emph{at least not strongly
  contradicted} by the data.

% from Royall: The 1/8 and 1/32 likelihood intervals are not confidence
% intervals, in general, but they truly represent what confidence intervals are
% often mistaken to represent, namely parameter values that the sample does not
% represent evidence against, that is, values that are ‘consistent with the
% observations’. We can speak in this way, asserting that there is not strong
% evidence against a point inside the interval, without reference to an
% alternative value, because the statement is true for all alternatives. Every
% point inside the 1/8 interval is consistent with the observations in the
% strong sense that there is no other possible value of the parameter that is
% better supported by a factor as large as 8. For points outside the likelihood
% intervals, the interpretation must be more careful. There is fairly strong
% evidence against a point just outside the 1 /8 likelihood interval in the
% specific sense that there is some alternative value, namely the maximum
% likelihood estimate (MLE) that is better supported by a factor of at least 8.


\begin{table*}[!htb]
    \centering
    \caption{Classifications of evidence for $\h{0}$ provided by $\BF_{01} = k$.
      The values of \citet{Jeffreys1961} are slightly adapted from powers of
      $\sqrt{10}$ as suggested by \citet{Held2018}. \citet{Fisher1956} defined
      his cutoffs relative to the likelihood of the data under the maximum
      likelihood estimate of the parameter $\theta$. He only named the
      $k < 1/15$ category, for the other categories the names from
      \citet{Shafer2021} are shown.}
    \label{tab:evidence}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{c c c c c c}
      \toprule
      $k$ & \citet{Jeffreys1961} & $k$ & \citet{Royall1997} & $k$ & \citet{Fisher1956} \\
      \cmidrule(lr){1-2}
      \cmidrule(lr){3-4}
      \cmidrule(lr){5-6}
      > 100 & Decisive & > 64 & Quite strong indeed & 1/2 to 1 & Good \\
      30 to 100 & Very strong & 32 to 64 & Quite strong & 1/5 to 1/2 & Fair \\
      10 to 30 & Strong & 8 to 32 & Pretty strong & 1/15 to 1/5 & Poor \\
      3 to 10 & Substantial & & & < 1/15 & Open to grave suspicion \\
      1 to 3 & Bare mention & & & & \\
      \bottomrule
    \end{tabular}%
    }
\end{table*}

The construction of support sets thus parallels the construction of frequentist
confidence sets: A $(1 - \alpha)100\%$ confidence set corresponds to the set of
parameter values which are not rejected by a null hypothesis significance test
at level $\alpha$. It can equally be displayed and obtained from a so-called
\emph{$p$-value function}, which is the $p$-value of the data viewed as a
function of the null value \citep{Fraser2019, Rafi2020}. Despite these
similarities, the interpretation of support and confidence sets is rather
different; support sets contain parameter values for which there is at least a
certain amount of statistical evidence, whereas confidence sets are defined
through the long-run frequency of including the unknown parameter $\theta$ with
probability equal to their confidence level. The parameter values in a
confidence sets are typically interpreted as being ``compatible'' with a
particular data set, but this is debatable as the confidence level is concerned
with the confidence set as a procedure over multiple replications.

In this article we shed light on the connection between support and confidence
sets. Specifically, we provide methods for calibrating approximate confidence
sets to approximate support sets and vice versa in the important case when the
data consists of an estimate of a univariate parameter $\theta$ with approximate
normal likelihood (Section~\ref{sec:SInormality}). Our main results are easy to
use formulas for computing support intervals that only require summary
statistics typically reported in research articles, \eg{} point estimates,
standard errors, or confidence intervals. Calibrating confidence to support
intervals requires the specification of a prior distribution for $\theta$ under
the alternative $\h{1}$, and we compare several classes of distributions. We
also show how bounding the evidence against the null hypothesis for a certain
class of prior distributions leads to so-called \emph{minimum support sets}. In
Section~\ref{sec:SIbounds}, we show how these minimum support sets provide
confidence sets an evidential interpretation with respect to certain classes of
priors. We then illustrate how the sample size of a future study can be
determined based on support, which provides an alternative to the conventional
approaches based on either power or precision of an interval estimator
(Section~\ref{sec:design}). Finally, we show how the universal bound for the
type-I error rate of Bayes factors can be used for bounding the coverage of
support sets, even under sequential analyses with optional stopping
(Section~\ref{sec:t1e}). As a running example, we use data from the RECOVERY
trial \citep{RECOVERY2021}, as already introduced in Figure~\ref{fig:example}.



\section{Support intervals under normality}
\label{sec:SInormality}
Denote by $\that$ an asymptotically normal estimator of an unknown univariate
parameter $\theta$, possibly the maximum likelihood estimator (MLE). Suppose its
squared standard error $\sigma^2$ is an estimate of the asymptotic variance of
$\that$, so that an approximate normal likelihood
$\that \given \theta \sim \Nor(\theta, \sigma^{2})$ is justifiable. Typically,
the standard error is of the form $\sigma = \lambda /\sqrt{n}$, where $n$ is the
effective sample size and $\lambda^2$ a variance corresponding to one effective
unit. An approximate $(1 - \alpha)100\%$ confidence interval for $\theta$ is
given by
\begin{align}
    \label{eq:ci}
    \that \pm \sigma \times \Phi^{-1}(1 - \alpha/2)
\end{align}
with $\Phi^{-1}(\cdot)$ the quantile function of the standard normal
distribution. The confidence level $(1 - \alpha)100\%$ represents the long run
frequency with which the true parameter is included in the confidence interval
(assuming that the sampling model is correct). Note that the
interval~\eqref{eq:ci} also corresponds to the $(1 -\alpha)$ posterior credible
interval based on an (improper) uniform prior for $\theta$, corresponding to
Jeffreys's transformation invariant prior \citep{Jeffreys1961, Ly2017} and thus
also representing the default interval estimate for $\theta$ from a Bayesian
estimation perspective. We will now contrast the confidence
interval~\eqref{eq:ci} to several types of support intervals.

\subsection{Normal prior under the alternative}
To construct a support interval for $\theta$, specification of a prior for
$\theta$ under the alternative $\h{1}$ is required. Specifying a normal prior
$\theta \given \h{1} \sim \Nor(\mu_\theta, \sigma_\theta^2)$ results in the
Bayes factor
% %% uncomment for short equation
% \begin{align}
%     \label{eq:bf01}
%   \BF_{01}(\that; \theta_0)
%   = \frac{\sqrt{1 + \sigma_\theta^2/\sigma^2}}{\exp\bigg[\dfrac{1}{2}\bigg\{\dfrac{(\that -
%   \that_0)^2}{\sigma^2} - \dfrac{(\that - \mu_\theta)^2}{\sigma^2 + \sigma_\theta^2}\bigg\} \bigg]}.
% \end{align}
%% uncomment for long equation
\begin{align}
    \label{eq:bf01}
  \BF_{01}(\that; \theta_0)
  = \sqrt{1 + \frac{\sigma_\theta^2}{\sigma^2}} ~ \exp\left[-\frac{1}{2}\left\{\frac{(\that -
  \that_0)^2}{\sigma^2} - \frac{(\that - \mu_\theta)^2}{\sigma^2 + \sigma_\theta^2}\right\} \right].
\end{align}
Now, fixing the Bayes factor~\eqref{eq:bf01} to $k$ and solving for $\theta_0$ leads
to the $k$ support interval
\begin{align}
    \label{eq:si}
    \that \pm \sigma \times
    \sqrt{\log\left(1 + \frac{\sigma_\theta^2}{\sigma^2}\right) + \frac{(\that - \mu_\theta)^2}{\sigma^2 + \sigma_\theta^2} -
    2\log k}.
\end{align}

Similar to the confidence interval~\eqref{eq:ci}, the support
interval~\eqref{eq:si} is centered around the parameter estimate $\that$.
However, while the width of the confidence interval is only determined through
the confidence level $(1 - \alpha)$ and standard error $\sigma$, the width of
the support interval also depends on the specified prior for $\theta$ under
$\h{1}$. Moreover, for $k > 1$ it may happen that the support interval does not
exist, as the term below the square root in~\eqref{eq:si} may become negative
for too large $k > 1$. This means that in order to find the desired level of
support $k > 1$, the data have to be sufficiently informative (relative to the
prior), \ie{} the squared standard error $\sigma^2$ has to be sufficiently small
relative to the prior variance $\sigma_\theta^2$.

In the following, we will discuss how different prior means $\mu_\theta$ and
variances $\sigma_\theta^2$ affect the resulting support intervals. When the
prior variance decreases ($\sigma_\theta^2 \downarrow 0$), the prior approaches
a point mass at $\mu_\theta$. The width of the support interval is then fully
determined by the difference between the parameter estimate $\that$ and the
prior mean $\mu_\theta$ divided by the standard error $\sigma$. A smaller
difference between $\that$ and $\mu_{\theta}$ leads to a tighter support
interval. In contrast, for priors that become increasingly diffuse
($\sigma_\theta^{2} \to \infty$), the $k \geq 1$ support interval~\eqref{eq:si}
extends to the entire real number line, indicating that all values
$\theta \in \R$ receive more support from the data than the diffuse alternative,
regardless of the data, i.e., the observed estimate $\that$, standard error
$\sigma$, and the location of the prior mean $\mu_\theta$. This particular
behavior provides another perspective on the well-known Jeffreys-Lindley paradox
\citep{Wagenmakers2021a}; the confidence interval from~\eqref{eq:ci} only spans
a finite range around the parameter estimate $\that$, so that the corresponding
null hypothesis significance tests would reject the parameter values outside,
whereas for the same values the Bayes factor would indicate evidence for the
null hypothesis. Finally, centering the prior around the parameter estimate
($\mu_\theta = \that$) and setting the prior variance equal to the variance of
one effective observation ($\sigma_\theta^2 = n \times \sigma^2$ with $n$ the
effective sample size), produces the support interval for Jeffreys's approximate
Bayes factor \citep{Wagenmakers2022}. In this case, the standard error
multiplier has a particularly simple form
$\mbox{M} = \surd\{\log(1 + n) - 2 \log k\}$, showing that at least
$n \geq k^2 - 1$ effective observations are required for the respective support
interval with $k \geq 1$ to exist.

\subsection{Local normal prior under the alternative}
\label{sec:local}
The support interval based on the normal prior~\eqref{eq:si} depends on the
specification of a prior mean and prior variance. A different approach is to use
a so-called \emph{local prior}, that is, a unimodal and symmetric prior centered
around the null value $\theta_0$ \citep{Berger1987b}.
% , for instance, the unit-information prior
% $\theta \given \h{1} \sim \Nor(\mu_\theta = \theta_0, \sigma_\theta^2 = n \times \sigma^2)$ from
% \citet{Kass1995b}.
Choosing a local normal prior with variance $\sigma^2_\theta$ corresponds to
setting $\mu_\theta = \theta_0$ in~\eqref{eq:si}, which leads to the Bayes
factor
% %% uncomment for short equation
% \begin{align}
%     \label{eq:bf01local}
%   \BF_{01}(\that ; \theta_0)
%   = \frac{\sqrt{1 + \sigma_\theta^2/\sigma^2}}{
%   \exp\bigg\{\dfrac{1}{2} \, \dfrac{(\that - \theta_0)^2}{\sigma^2(1 + \sigma^2/\sigma_\theta^2)}
%   \bigg\}}.
% \end{align}
%% uncomment for long equation
\begin{align}
    \label{eq:bf01local}
  \BF_{01}(\that ; \theta_0)
  = \sqrt{1 + \frac{\sigma_\theta^2}{\sigma^2}}
  ~ \exp\left\{-\frac{1}{2} \, \frac{(\that - \theta_0)^2}{\sigma^2(1 + \sigma^2/\sigma_\theta^2)} \right\}.
\end{align}
The $k$ support interval based on the Bayes factor~\eqref{eq:bf01local} is then
given by
\begin{align}
    \label{eq:silocal}
    \that \pm \sigma \times
    \sqrt{\left\{\log\left(1 + \frac{\sigma_\theta^2}{\sigma^{2}}\right) - 2\log k \right\}
    \left(1 + \frac{\sigma^{2}}{\sigma_\theta^2}\right)}.
\end{align}

While the Bayes factor~\eqref{eq:bf01local} is a special case of the Bayes
factor~\eqref{eq:bf01}, the support interval~\eqref{eq:silocal} is not a special
case of the support interval~\eqref{eq:si}. This is because the prior for
$\theta$ under $\h{1}$ is different for each null value $\theta_0$, whereas it
is always the same under the two-parameter normal prior approach. To fully
specify the support interval~\eqref{eq:silocal}, the prior variance
$\sigma_\theta^{2}$ needs to be chosen. One standard choice is to set it equal
to the variance of a single observation ($\sigma_\theta^2 = n \times \sigma^2$),
known as unit-information prior \citep{Kass1995b}. This approach leads to the
$k$ support interval
\begin{align}
    \label{eq:silocalui}
    \that \pm \sigma \times
    \sqrt{\left\{\log\left(1 + n\right) - 2\log k \right\}
    \left(1 + 1/n\right)}.
\end{align}
For this type of support interval, the standard error multiplier
$\mbox{M} = \surd[\{\log(1 + n) - 2 \log k\}(1 + 1/n)]$ is wider than for the
Jeffreys's approximate Bayes factor by a factor of $\surd(1 + 1/n)$ but the
condition $n \geq k^2 - 1$ for the existence of the $k \geq 1$ support interval
is the same.

<< "existence-nonlocal-SI" >>=
## W_0 function has to be larger than 1/2 for support interval to exist
rootFun <- function(x) lamW::lambertW0(x = x) - 0.5
lArg <- uniroot(f = rootFun, interval = c(0, exp(1)))$root
@

\subsection{Nonlocal normal moment prior under the alternative}
\label{sec:nonlocal}
Another attractive class of priors for $\theta$ under the alternative is given
by so-called \emph{nonlocal priors}. These priors are characterized by having no
density at the null value $\theta_0$, thereby leading to a faster accumulation
of evidence than local priors when the null hypothesis is actually true
\citep{Johnson2010}. One popular type of nonlocal priors is given by
\emph{normal moment priors} $\theta \sim \mathrm{NM}(\theta_0, \sigma_\theta)$,
with symmetry point $\theta_0$ and scale $\sigma_\theta$ which have density
$f(\theta \given \theta_0, \sigma_\theta^2) = \Nor(\theta \, ; \, \theta_0, \sigma_\theta^2) \times (\theta - \theta_0)^{2}/\sigma_\theta^2$
where $\Nor(\cdot \, ; \, \theta_0, \sigma_\theta^2)$ denotes the density
function of a normal distribution with mean $\theta_0$ and variance
$\sigma_\theta^2$. The Bayes factor employing a prior
$\theta \given \h{1} \sim \mathrm{NM}(\theta_0, \sigma_\theta^2)$ is then given
by
% %% uncomment for short equation
% \begin{align*}
%   \BF_{01}(\that ; \theta_0)
%   =&
%      \left(1 + \frac{\sigma_\theta^2}{\sigma^2}\right)^{3/2} \,
%      \exp\left\{-\frac{1}{2} \, \frac{(\that - \theta_0)^2}{\sigma^2(1 + \sigma^2/\sigma_\theta^2)}
%      \right\} \\
%    & \times \left(1 + \frac{(\that - \theta_0)^2}{\sigma^2(1 + \sigma^2/\sigma_\theta^2)}\right)^{-1}
%      \nonumber
% \end{align*}
%% uncomment for long equation
\begin{align*}
  \BF_{01}(\that ; \theta_0)
  = \left(1 + \frac{\sigma_\theta^2}{\sigma^2}\right)^{3/2} \,
  \exp\left\{-\frac{1}{2} \, \frac{(\that - \theta_0)^2}{\sigma^2(1 + \sigma^2/\sigma_\theta^2)} \right\}
  \left(1 + \frac{(\that - \theta_0)^2}{\sigma^2(1 + \sigma^2/\sigma_\theta^2)}\right)^{-1}
\end{align*}
from which the corresponding $k$ support interval can be derived to be
% %% uncomment for short equation
% \begin{align}
%   \label{eq:sinonlocal}
%   \that \pm \sigma \times
%   \sqrt{\dfrac{2 \lw{0}\left\{\dfrac{(1 + \sigma_\theta^2/\sigma^2)^{3/2}
%   \sqrt{e}}{2 k}\right\} - 1}{
%   \left(1 + \sigma^2/\sigma^2_\theta\right)^{-1}}}
% \end{align}
%% uncomment for long equation
\begin{align}
  \label{eq:sinonlocal}
  \that \pm \sigma \times
  \sqrt{\left[2 \lw{0}\left\{\frac{(1 + \sigma_\theta^2/\sigma^2)^{3/2} \sqrt{e}}{2 k}\right\} - 1\right]
  \left(1 + \frac{\sigma^2}{\sigma_\theta^2}\right)}
\end{align}
with $\lw{0}(\cdot)$ denoting the principal branch of the Lambert W function
\citep{Corless1996}. It is possible that the support
interval~\eqref{eq:sinonlocal} does not exist, as for the other two types of
support intervals. This happens when the Lambert W term is smaller than one half
so that the square root is undefined. Since
$\lw{0}(\Sexpr{round(lArg, 2)}) \approx 1/2$, this situations occurs when
$(1 + \sigma_\theta^2/\sigma^2)^{3/2} < \Sexpr{round(lArg, 2)} \times 2k\sqrt{e}$,
meaning that the standard error $\sigma$ has to be sufficiently small relative
to the prior scale $\sigma_\theta$ parameter and the support level $k$, so that
the interval exists.


\section{Support intervals based on Bayes factor bounds}
\label{sec:SIbounds}
In some situations it is clear which prior for $\theta$ should be chosen under
the alternative $\h{1}$, \eg{} when a parameter estimate from a previous data
set is available. In other situations it is less clear and different priors may
produce drastically different results. To provide a more objective assessment of
evidence in the latter situation, several authors have proposed to instead
specify only a class of prior distributions and then select the one prior among
them that leads to the Bayes factor providing the most possible evidence against
the null hypothesis $\h{0}$ \citep{Edwards1963, Berger1987, Selke2001,
  Held2018}. Here we refer to these Bayes factor bounds as \emph{minimum Bayes
  factors} for the null $\h{0}$ over the alternative $\h{1}$, as we are
interested in the support for null values $\theta_0$.

We will now show how minimum Bayes factors can be used for obtaining so-called
\emph{minimum support sets}. Specifically, a $k$ minimum support set is given by
\begin{align}
  \mbox{minSI}_k =
  \left\{\theta_0 : \minBF_{01}(x ; \theta_0) \geq k \right\},
\end{align}
where $\minBF_{01}(x ; \theta_0)$ is the smallest possible Bayes factor for
testing $\h{0}\colon \theta = \theta_0$ versus
$\h{1}\colon \theta \neq \theta_0$ that can be obtained from a class of prior
distributions for $\theta$ under the alternative $\h{1}$. That is, given the
data, for each $\theta_0$ the prior for $\theta$ under $\h{1}$ is cherry-picked
from a class of priors to obtain the lowest evidence for
$\h{0}\colon \theta = \theta_0$ possible. Minimum support intervals thus provide
a Bayes/non-Bayes compromise \citep{Good1992} as they do not require
specification of a specific prior distribution but still allow for an evidential
interpretation of the resulting interval.

One property of minimum Bayes factors is that they can only be used to asses the
maximum evidence \emph{against} the null hypothesis but not for it. Minimum
support sets inherit this property, meaning that they can only be obtained for
support levels $k \leq 1$. For instance a $k = 1/3$ minimum support set includes
the parameter values under which the observed data are \emph{at most} $3$ times
less likely compared to under all priors from the specified class of
alternative. Being unable to obtain support intervals with $k > 1$ is the price
that needs to be paid for having to only specify a class of prior distributions
but not a specific prior itself. We will now discuss minimum support intervals
from several important classes of distributions.

\subsection{Class of all distributions under the alternative}

Among the class of all possible priors under $\h{1}$, the prior which is most
favorable towards the alternative is a point mass at the observed effect
estimate $\h{1}\colon \theta = \that$ \citep{Edwards1963}. The resulting minimum
Bayes factor is given by
\begin{align}
    \label{eq:minBFsimple}
  \minBF_{01}(\that ; \theta_0) =
  \exp\left\{-\frac{1}{2}  \frac{(\that - \theta_0)^{2}}{\sigma^{2}}\right\},
\end{align}
which equals the standard likelihood ratio test statistic when $\that$ is the
MLE. Inverting~\eqref{eq:minBFsimple} for $\theta_0$ leads to the $k$ minimum
support interval
\begin{align}
    \label{eq:sisimple}
  \that \pm \sigma \times \sqrt{-2\log k}.
\end{align}
Interestingly, defining a support interval relative to the likelihood of the
data under the MLE has already been suggested by \citet{Fisher1956}.
Table~\ref{tab:evidence} shows his classification of evidence for this type of
interval. It is, however, important to note that from a Bayesian perspective the
support interval~\eqref{eq:sisimple} represents the most blatantly biased
assessment of support in the sense that assigning a point prior at the observed
parameter estimate hardly reflects prior knowledge about $\theta$ but can rather
be considered cheating \citep{Berger1987}. This is reflected by the fact that
for a given estimate (i.e., data set) and fixed support level $k$, the interval
represents the narrowest support interval possible among all possible support
intervals. When minimizing over the class of all two-parameter normal priors,
i.e., the Bayes factor~{\eqref{eq:bf01}}, we get the same minimum Bayes
factor~{\eqref{eq:minBFsimple}}.



\subsection{Class of local normal alternatives}

When the class of priors for $\theta$ under the alternative $\h{1}$ is given by
normal distributions centered around the null value $\theta_0$, choosing its
variance to be $\sigma_\theta^2 = \max\{(\that - \theta_0)^2 - \sigma^2, 0\}$
maximizes the marginal likelihood of the data under $\h{1}$. Plugging this
variance in the Bayes factor~\eqref{eq:bf01local} leads to the minimum Bayes
factor over the class of local normal priors
%% uncomment for long equation
\begin{align}
  \minBF_{01}(\that ; \theta_0) =
  \begin{cases}
    \dfrac{|\that - \theta_0|}{\sigma} \exp\left\{-\dfrac{(\that -
    \theta_0)^2}{2\sigma^{2}}\right\}  \sqrt{e}
    & ~ \text{if} ~ \dfrac{|\that - \theta_0|}{\sigma} > 1 \\
    1 & ~ \text{else}
  \end{cases}
  \label{eq:minBFnorm}
  \end{align}
% %% uncomment for short equation
% \begin{align}
%   \minBF_{01}(\that ; \theta_0) =
%   \begin{cases}
%     |z_{\scriptscriptstyle \theta_0}| \, \exp\left\{
%     -\dfrac{z_{\scriptscriptstyle \theta_0}^2 - 1}{2}\right\}
%     & ~ \text{if} ~ z_{\scriptscriptstyle \theta_0} > 1 \\
%     1 & ~ \text{else}
%   \end{cases}
%   \label{eq:minBFnorm}
% \end{align}
% with $z_{\scriptscriptstyle \theta_0} = (\that - \theta_0)/\sigma$,
as first shown by \citet{Edwards1963}. Equating~\eqref{eq:minBFnorm} to $k$ and
solving for $\theta_0$ leads then to the $k$ minimum support interval
\begin{align}
  \that \pm \sigma \times \sqrt{-\lw{-1}(-k^2/e)},
\end{align}
with $\lw{-1}(\cdot)$ the branch of the Lambert W function that satisfies
$\mbox{W}(y) \leq -1$ for $y \in [-e^{-1}, 0)$ \citep{Corless1996}. For $k = 1$,
the standard error multiplier becomes $\mbox{M} = \sqrt{-\lw{-1}(-1/e)} = 1$.
Hence, the data provide support for all parameter values within one standard
error around the observed parameter estimate $\that$ when the class of priors
for the parameter is given by local normal alternatives.


\subsection{Class of \textit{p}-based alternatives}

\citet{Vovk1993} and \citet{Selke2001} proposed a minimum Bayes factor where the
data are summarized through a $p$-value. The idea is that under the null
hypothesis $\h{0}\colon \theta = \theta_0$, a $p$-value should be uniformly
distributed, whereas under the alternative it should have a monotonically
decreasing density characterized by the class of Beta($\xi, 1$) distributions
(with $\xi \geq 1$). Choosing $\xi$ such that the marginal likelihood of the
data under $\h{1}$ is maximized, leads to well-known ``$-ep \log p$'' minimum
Bayes factor
\begin{align}
  \minBF_{01}(p ; \theta_0) =
  \begin{cases}
    - e  p  \log p
    & ~ \text{if} ~ p \leq e^{-1} \\
    1 & ~ \text{else}
  \end{cases}
  \label{eq:minBFeplog}
\end{align}
with $p = 2\{1 - \Phi(|\that - \theta_0|/\sigma)\}$.
Equating~\eqref{eq:minBFeplog} to $k$ and solving for $\theta_0$, leads to the
$k$ minimum support interval
\begin{align}
  \that \pm \sigma \times \Phi^{-1}\left[1 -
  \frac{\exp\left\{\lw{-1}(-k/e)\right\}}{2}\right].
\end{align}
For $k = 1$, the standard error multiplier is given by
$\mbox{M} = \Phi^{-1}[1 - \exp\{\lw{-1}(-1/e)\}/2] = \Phi^{-1}[1 - 1/(2e)] \approx 0.90$,
so the $k = 1$ minimum support interval is just slightly tighter than the one
based on local normal alternatives.


%% place at appropriate place in preprint/journal
<< "mapping-conf-minsupport" >>=
## functions to compute minimum support level k from confidence level such that
## the same standard error multiplier
## -----------------------------------------------------------------------------
## class of all priors
conf2kall <- function(conf) {
    exp(-0.5*qnorm(p = (1 + conf)*0.5)^2)
}
## class of local normal priors
conf2knormallocal <- function(conf) {
    z <- qnorm(p = (1 + conf)*0.5)
    abs(z)*exp(-0.5*(z^2 - 1))
}
## -eplogp
conf2keplogp <- function(conf) {
    k <- -exp(1)*log(1 - conf)*(1 - conf)
    return(k)
}
conf2k <- list(conf2kall, conf2keplogp, conf2knormallocal)
conf2kName <- c("italic(k) ~ 'minimum support (all priors)'",
                "italic(k) ~ 'minimum support ('*-italic('e') *italic('p') * 'log' *italic('p')* ')'",
                "italic(k) ~ 'minimum support (local normal priors)'")

## which k minimum SI correspond to 95% CI?
k95 <- sapply(X = conf2k, FUN = function(f) f(0.95))

## functions to compute confidence level from  minimum support level such that
## the same standard error multiplier
## -----------------------------------------------------------------------------
## class of all priors
k2confall <- function(k) {
    2*pnorm(q = sqrt(-2*log(k))) - 1
}
## class of local normal priors
k2confnormallocal <- function(k) {
    2*pnorm(q = sqrt(-lambertWm1(x = -k^2/exp(1)))) - 1
}
## -eplogp
k2confeplogp <- function(k) {
    2*pnorm(q = qnorm(p = 1 - exp(lambertWm1(-k/exp(1)))/2)) - 1
}
k2conf <- list(k2confall, k2confeplogp, k2confnormallocal)

## which confidence level corresponds to k = 1/10 minimum support level?
conf10 <- sapply(X = k2conf, FUN = function(f) f(1/10))
@


\subsection{Mapping between confidence and minimum support levels}
For all types of minimum support intervals discussed so far, there is a
one-to-one mapping between their minimum support level $k$ and the confidence
level $(1 - \alpha)\%$ of the approximate confidence interval~\eqref{eq:ci}, see
Figure~\ref{fig:mappingCIk}. The conventional default level of 95\% corresponds
to a $k = \Sexpr{formatBF(k95[1])}$ support level for the class of all priors
under the alternative, a $k = \Sexpr{formatBF(k95[2])}$ support level for the
$-ep\log p$, and a $k = \Sexpr{formatBF(k95[3])}$ support level for the local
normal prior calibration. Conversely, the $k = 1/10$ minimum support interval
corresponds to the $\Sexpr{round(100*conf10[1], 2)}\%$ confidence interval for
the class of all priors, the $\Sexpr{round(100*conf10[2], 2)}$\% confidence
interval for $-ep\log p$, and the $\Sexpr{round(100*conf10[3], 2)}$\% confidence
intervals for the local normal prior calibration. Similarly as the mappings
between Bayes factor bounds and $p$-values \citep{Held2018}, the mappings
displayed in Figure~\ref{fig:mappingCIk} provides confidence intervals an
evidential interpretation. Specifically, it enhances their long-term frequency
interpretation with an interpretation that directly relates to the minimum
support that \emph{the observed data} provide for the parameter values in the
interval.

\begin{figure*}[!htb]
<< "figure-mapping-conf-minsupport", fig.height = ifelse(type == "journal", 2.8, 3.3) >>=
## computing mapping for a grid of confidence levels
confseq <- seq(0.8, 0.99999, length.out = 2000)
mseq <- qnorm(p = 0.5*(1 + confseq))
plotDF <- do.call("rbind", lapply(X = seq_along(conf2k), FUN = function(i) {
    f <- conf2k[[i]]
    k <- f(conf = confseq)
    data.frame(type = conf2kName[i], conflevel = confseq, m = mseq, k = k)
}))
plotDF$type <- factor(x = plotDF$type, levels = conf2kName)

## plotting conflevel vs minimum support level
bfBks <- c(1/300, 1/100, 1/30, 1/10, 1/3, 1)
confbks <- seq(50, 100, 0.5)
ggplot(data = plotDF, aes(x = conflevel*100, y = k, color = type)) +
    geom_line(alpha = 0.8, size = 0.7) +
    ## scale_x_continuous(sec.axis = sec_axis(~ conf2z(./100), name = "SE multiplier",
    ##                                        breaks = c(1.7, 1.8, 2, 2.3, 3))) +
    scale_x_continuous(breaks = confbks) +
    scale_y_log10(breaks = bfBks, labels = formatBF(BF = bfBks),
                  expand = c(0, 0)) +
    expand_limits(y = 1.5) +
    coord_cartesian(xlim = c(95, 99.9), ylim = c(1/250, 1.3)) +
    labs(y = bquote("Minimum support level" ~ italic(k)),
         x = bquote("Confidence level" ~ (1 - alpha)*"%"),
         color = "") +
    scale_color_brewer(palette = "Dark2", labels = scales::parse_format()) +
    theme_bw() +
    theme(legend.position = "top", panel.grid.minor = element_blank(),
          legend.text = element_text(size = 7.5))
@
\caption{Mapping between confidence level $(1 - \alpha)\%$ and minimum support
  level $k$ for different types of minimum support intervals.}
    \label{fig:mappingCIk}
  \end{figure*}


<< "eliciation-scale-nonlocal" >>=
## elicitation of a suitable value for the scale parameter of the nonlocal
## moment prior as in Pramanik and Johnson (2022)
## -----------------------------------------------------------------------------

## density function of normal moment prior centered around m with scale s
dnormMoment <- function(x, m, s) {
    dnorm(x = x, mean = m, sd = s) * (x - m)^2/s^2
}
## cumulative distribution function of normal moment prior centered around m
## with scale s
pnormMoment_ <- function(q, m, s, lower.tail = TRUE) {
    p <- integrate(f = dnormMoment, lower = -Inf, upper = q, m = m, s = s)$value
    if (lower.tail == FALSE) p <- 1 - p
    return(p)
}
pnormMoment <- Vectorize(FUN = pnormMoment_)

## determine scale s such that 90% probability within [m - log2, m + log2]
pnonlocal <- 0.9
rootFun <- function(s) {
    p <- pnormMoment(q = log(2), m = 0, s = s) -
        pnormMoment(q = -log(2), m = 0, s = s)
    return(p - pnonlocal)
}
snonlocal <- uniroot(f = rootFun, lower = 0.1, upper = 10)$root
vnonlocal <- snonlocal^2

## ## look at resulting prior
## xseq <- seq(-1.5, 1.5, 0.001)
## plot(xseq, dnormMoment(x = xseq, m = 0, s = sNonlocal), type = "l",
##      xlab = bquote("Log hazard ratio" ~ theta), ylab = "Density")
@

\section{Example RECOVERY trial}
We now compute the above (minimum) support intervals for the data from the
RECOVERY trial \citep{RECOVERY2021}. With the standard error $\sigma$ known, the
minimum support intervals are fully specified and can be readily For the normal,
local normal, and the nonlocal normal moment prior we choose their parameters as
follows. The trial steering committee determined the sample size of the trial
based on an assumed clinically relevant log hazard ratio of
$\log \Sexpr{exp(m)} = \Sexpr{round(m, 2)}$. This effect size can be used to
inform the normal prior under the alternative $\h{1}$, \ie{} we specify the mean
$\mu_\theta = \Sexpr{round(m, 2)}$ along with the unit-information variance
$\sigma_\theta^2 = \Sexpr{round(v, 2)}$ for a log hazard ratio \citep[ch.
2.4.2]{Spiegelhalter2004}. Likewise, we use the unit-information variance
$\sigma_\theta^2 = \Sexpr{round(v, 2)}$ as the variance of the local normal
prior. The scale parameter of the nonlocal moment prior $\sigma_{\theta}$ is
elicited with a similar approach as in \citet{Pramanik2022}; The value
$\sigma_\theta = \Sexpr{round(snonlocal, 2)}$ is selected so that
\Sexpr{round(100*pnonlocal, 2)}\% probability mass is assigned to log hazard
ratios between $\theta_0 - \log2$ and $\theta_0 + \log 2$, representing effect
sizes that at most half or double the mortality hazards relative to the null
value $\theta_0$.

<< "compute-SI-RECOVERY" >>=
## compute different types of support intervals for different support levels k
## -----------------------------------------------------------------------------

## set up grid of methods and support levels
k <- c(1/10, 1/3, 1, 3, 10)
methods <- c("SI-normal","SI-normal-local","SI-normal-nonlocal", "mSI-all",
             "mSI-normal-local", "mSI-eplogp")
methodName <- c("italic(k) ~ 'SI (normal prior)'",
                "italic(k) ~ 'SI (local normal prior)'",
                "italic(k) ~ 'SI (nonlocal normal moment prior)'",
                "italic(k) ~ 'minSI (all priors)'",
                "italic(k) ~ 'minSI (local normal priors)'",
                "italic(k) ~ 'minSI ('*-italic('e') *italic('p') * 'log' *italic('p')* ')'")
names(methods) <- methodName
ciName <- "'95% CI'"
typeLevels <- rev(c(ciName, methodName))
applyGrid <- expand.grid(k = k, methodi = seq(1, length(methods)))
applyGrid$method <- methods[applyGrid$methodi]
applyGrid$name <- methodName[applyGrid$methodi]

## compute SIs
m <- log(0.8) # mean of normal prior
v <- 4 # unit information variance normal and local normal prior
vnonlocal <- 0.28^2 # scale parameter of nonlocal normal prior
siDF <- do.call("rbind", lapply(X = seq(1, nrow(applyGrid)), FUN = function(i) {
    k <- applyGrid$k[i]
    method <- applyGrid$method[i]
    name <- applyGrid$name[i]
    ## HACK prior parameters are ignored for methods that do not depend on them
    ## so we can set priorMean = m for all methods since only "SI-normal"
    ## depends on it. Is there a cleaner solution?
    if (method == "SI-normal-nonlocal") {
        priorSD <- sqrt(vnonlocal)
    } else {
        priorSD <- sqrt(v)
    }
    si <- ciCalibrate(estimate = logHR, se = selogHR, method = method,
                      siLevel = k, priorMean = m, priorSD = priorSD)
    data.frame(k = k, method = method, type = name, lower = si$si[1],
               upper = si$si[2], est = logHR)
}))

## combine SI data frame with CI
ciDF <- data.frame(k = NA, method = "CI", type = ciName,
                   lower = log(ciHR)[1], upper = log(ciHR[2]), est = logHR)
plotDF <- rbind(siDF, ciDF)
plotDF$type <- ordered(x = plotDF$type, levels = typeLevels)

## select some SI to discuss in the text
si10norm <- subset(x = plotDF, k == 10 & method == "SI-normal")
si10normlocal <- subset(x = plotDF, k == 10 & method == "SI-normal-local")
si10normnonlocal <- subset(x = plotDF, k == 10 & method == "SI-normal-nonlocal")
@

\begin{figure*}[!htb]
<< "figure-SIcomparison-RECOVERY", fig.height = ifelse(type == "journal", 2.8, 3.25) >>=
## define diverging color palette
plotDF$kfac <- factor(plotDF$k, levels = k, labels = formatBF(BF = k))
cols <- divergingx_hcl(n = length(k), palette = "Zissou 1", rev = TRUE)
names(cols) <- levels(plotDF$kfac)

## plot CI and SIs
plotA <- ggplot() +
    geom_point(data = plotDF, aes(x = type, y = est), size = 0, alpha = 0) +
    geom_errorbar(data = ciDF, aes(x = type, ymin = lower, ymax = upper),
                  col = 1, alpha = 1, width = 0.35, size = 0.4)
## HACK draw SIs in correct order so that narrower SI on top of longer ones
for (i in seq_along(k)) {
    ki <- k[i]
    dat <- subset(plotDF, k == ki)
    plotA <- plotA +
        geom_errorbar(data = dat, key_glyph = "rect",  alpha = 1,
                      size = 0.4 + i*0.02,
                      aes(x = type, ymin = lower, ymax = upper, color = kfac),
                      width = 0.35)
}
plotA +
    geom_point(data = subset(plotDF, type == ciName),
               aes(x = type, y = est), size = 0.9, color = 1, alpha = 1) +
               #shape = "square") +
    labs(x = "", y = bquote("Log hazard ratio" ~ theta),
         color = bquote("Support level" ~ italic(k))) +
    scale_x_discrete(labels = function(l) parse(text = l)) +
    scale_color_manual(values = cols) +
    coord_flip() +
    theme_bw() +
    theme(legend.position = "top",
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          axis.text.y = element_text(hjust = 0, color = "black"))
@

\caption{Comparison of different support intervals for data from RECOVERY trial.
  The normal prior is centered around the clinically relevant proportional
  mortality reduction $\mu_{\theta} = \log \Sexpr{exp(m)} = \Sexpr{round(m, 2)}$
  (as deemed by the trial steering committee) and has unit variance
  $\sigma_\theta^{2} = \Sexpr{round(v, 2)}$. The local normal prior also has
  unit variance $\sigma_\theta^{2} = \Sexpr{round(v, 2)}$. The scale parameter
  of the nonlocal normal moment prior is
  $\sigma_\theta = \Sexpr{round(snonlocal, 2)}$ so that
  \Sexpr{round(100*pnonlocal, 2)}\% of the probability is assigned to log hazard
  ratios between $\theta_0 - \log2$ and $\theta_0 + \log 2$, representing effect
  sizes that at most half or double the mortality hazards relative to the null
  value $\theta_0$.}
    \label{fig:comparison}
\end{figure*}

Figure~\ref{fig:comparison} shows the corresponding $k$ support intervals for
different values of $k$. The support intervals based on normal (second row) and
local normal prior (third row) mostly coincide for all considered support levels
$k$.
% This is because the mean $m = -0.22$ of the prior under $\h{1}$ is close to the
% observed effect estimate $\that = -0.19$, so that
The $k = 10$ support intervals (blue) from both types indicate that log hazard
ratios between $\Sexpr{signif(si10norm[,"lower"], 2)}$ and
$\Sexpr{round(si10norm[,"upper"], 2)}$ receive strong support from the data
compared to alternative parameter values. In contrast, the $k = 10$ support
interval (blue) based on the nonlocal normal moment prior (fourth row) is
slightly wider, indicating that values between
$\Sexpr{signif(si10normnonlocal[,"lower"], 2)}$ and
$\Sexpr{round(si10normnonlocal[,"upper"], 2)}$ are strongly supported by the
data. For smaller support levels ($k < 10$) this trend reverses and the normal
and local normal prior support intervals are wider than the one based on the
nonlocal normal prior. Finally, each parameter value not included in a $k$
support interval corresponds to a point-null hypothesis for which the respective
Bayes factor is smaller than $k$, similarly to the relationship between
confidence intervals and $p$-values. For instance, one can immediately see that
the Bayes factor based on nonlocal moment priors indicates strong evidence
($\BF_{01} < 1/10$) against $\h{0}\colon\theta = 0$ as the value is not included
in the interval, whereas this is not the case for the Bayes factors based on
normal and local normal priors.


The three bottom rows in Figure~\ref{fig:comparison} show different types of $k$
minimum support intervals computed for the data from the RECOVERY trial. Since
minimum support intervals only exist for $k \leq 1$, only such support levels
are shown. The (yellow) $k=1$ minimum support interval for the class of all
priors (fifth row) is just a point at the observed effect estimate
$\that = \Sexpr{round(logHR, 2)}$. In contrast, the (yellow) $k = 1$ minimum
support intervals based on local normal priors (sixth row) and the $-ep \log p$
calibration (last row) span about one standard error around the effect estimate.
Also for $k = 1/3$ (orange) and $k = 1/10$ (red), the minimum support interval
based on the class of all priors is much more narrow than the ones based on
local normal and $-ep\log p$, yet all of them are more narrow than the ordinary
support intervals. This illustrates that minimum support intervals provide an
overly pessimistic assessment of support for parameter values, in the same way
that Bayes factor bounds provide an overly pessimistic quantification of
evidence for the null hypothesis.


<< "sample-size-based-on-support" >>=
## determine sample size such that k SI based on JAB has width w
k <- 10
w <- 0.2
lambda <- 2
expterm1 <- lamW::lambertW0(x = -k^2*w^2/4/lambda^2)
expterm2 <- lamW::lambertWm1(x = -k^2*w^2/4/lambda^2)
n <- k^2*exp(-c(expterm1, expterm2))

## ## check that both solutions correct
## estCheck <- 0
## seCheck <- lambda/sqrt(n)
## par(mfrow = c(1, 2))
## plot(ciCalibrate(siLevel = k, estimate = estCheck, se = seCheck[1],
##                  priorMean = estCheck, priorSD = lambda))
## plot(ciCalibrate(siLevel = k, estimate = estCheck, se = seCheck[2],
##                  priorMean = estCheck, priorSD = lambda))
@

\section{Design of new studies based on support}
\label{sec:design}
The sample size of a future study is typically derived to achieve (i) a targeted
power of a hypothesis test, or (ii) a targeted precision of a future
confidence/credible interval. Here, we provide an alternative where the sample
size of a future study is determined to achieve a desired level of support.

Assume we wish to conduct a study and analyze the resulting parameter estimate
$\that$ using the support interval based on a normal prior~\eqref{eq:si}.
Further assume that we either specify a reasonable prior from existing knowledge
or use the prior for Jeffreys's approximate Bayes factor. The goal is now to
determine the sample size $n$ such that we can identify the parameter values
which are strongly supported by the future data, for instance, with a support
level $k = 10$ representing ``strong'' support in the classification from
\citet{Jeffreys1961}. In order for the $k > 1$ support interval~\eqref{eq:si} to
exist, the standard error $\sigma$ of the parameter estimate $\that$ needs to be
sufficiently small so that the term in the square root becomes non-negative,
\ie{} it must hold that
\begin{align}
  \label{eq:existence}
  \log\left(1 + \frac{\sigma_\theta^2}{\sigma^2}\right) + \frac{(\that - \mu_{\theta})^{2}}{\sigma^2 + \sigma_\theta^2} \geq 2 \log k.
\end{align}
The sample size $n$ can now be determined such that the standard error $\sigma$
is small enough for~\eqref{eq:existence} to hold. The resulting sample size then
guarantees that parameter values with the desired level of support will be
identified. In general, this needs to be done numerically, but for the Jeffrey's
approximate Bayes factor prior ($\mu_{\theta} = \that$ and
$\sigma_\theta^2 = n \sigma^2$), the simple expression $n \geq k^2 - 1$
mentioned earlier exists. For instance, if we want a $k = 10$ support interval
to exist, we must take at least $10^2 - 1 = 99$ samples.

While the previously described approach guarantees that a $k > 1$ support
interval exists for at least one parameter value $\theta$, one may want to
guarantee that the resulting $k$ support interval will span a desired length
\begin{align}
    \label{eq:width}
    \ell = 2 \sigma \times \mbox{M}_k,
\end{align}
with $\mbox{M}_k$ the standard error multiplier of a $k$ support interval. In
general, numerical methods are required for computing the $n$ such
that~\eqref{eq:width} is satisfied, yet again for the support interval based on
Jeffrey's approximate Bayes factor
%($\mbox{M}_k = \{\log(1 + n) - 2\log k\}$)
there are explicit solutions available
\begin{align}
    \label{eq:nw}
    n = k^2 \, \exp\left\{-\lw{}\left(-\frac{k^2 \ell^2}{4 \lambda^2}\right)\right\}
\end{align}
with $\lambda^2$ the variance of one (effective) observation and assuming
$\log(1 + n)/\log(n) \approx 1$. From~\eqref{eq:nw} two things are apparent: (i)
the argument to $\mbox{W}(\cdot)$ has to be larger than $-1/e$ for the function
value to be defined, meaning that the possible width is limited by
$\ell \leq (4\lambda^2)/k^2$, (ii) since the argument to $\mbox{W}(\cdot)$ is
negative, there are always two solutions given by the two real branches of the
Lambert $\mbox{W}$ function, if any exist at all. For instance, for a standard
error of $\sigma^{2} = \lambda^{2}/n$ with
$\lambda^{2} = \Sexpr{round(lambda^2, 2)}$, a support level
$k = \Sexpr{formatBF(k)}$, and a desired width $\ell = \Sexpr{round(w, 2)}$,
equation~\eqref{eq:nw} leads to the sample sizes $n_1 = \Sexpr{ceiling(n[1])}$
and $n_2 = \Sexpr{ceiling(n[2])}$ (when rounded to the next larger integer).
Both lead to the $k = \Sexpr{formatBF(k)}$ support interval spanning the desired
width $\ell = \Sexpr{round(w, 2)}$, yet for the study employing the larger
sample size $n_2$ other support intervals with higher support levels $k$ can be
computed compared to a study employing the smaller sample size $n_1$.



\section{Error control via the universal bound}
\label{sec:t1e}
The universal bound \citep[ch. 1.4]{Royall1997} ensures that for $k < 1$ and
when the null hypothesis $\h{0} \colon \theta = \theta_0$ is true, the
probability for finding evidence at level $k$ for $\h{0}$ cannot be larger than
$k$, that is
\begin{align}
    \label{eq:ubound}
  \P\left\{\BF_{01}(x ; \theta_0) \leq k \given \h{0}\right\} \leq k
\end{align}
for any prior of $\theta$ under the alternative $\h{1}$. Remarkably, the
universal bound is also valid under sequential analyses with optional stopping
as soon as a Bayes factor smaller than $k$ is obtained (\citet{Robbins1970,
  Pace2020}). In contrast, frequentist tests and confidence sets typically have
to be adjusted for sequential analyses to guarantee appropriate error rates, and
the theory and applicability can become quite involved.

\citet{Lindon2020} proved that $k$ support sets with $k < 1$ are also valid
$(1 - k)100\%$ confidence sets. Their proof and the related ``safe and anytime
valid inference'' theory \citep[see \eg{}][]{Grundwald2019} is based on
relatively technical results from martingale theory. We now briefly show how the
universal bound can also be used to derive error rate guarantees for support
intervals. Assume there is a true parameter $\theta = \theta_*$. For any
(data-independent) prior for $\theta$ under the alternative hypothesis $\h{1}$,
the coverage of the corresponding $k$ support set $\mbox{SI}_k$ with $k < 1$ is
bounded by
\begin{align}
    \P\left(\mbox{SI}_k \ni \theta_* \given \theta = \theta_*\right)
    &= \P\left\{\BF_{01}(x ; \theta_*) \geq k \given \theta = \theta_*\right\}  \nonumber \\
    &= 1 -  \P\left\{\BF_{01}(x ; \theta_*) < k \given \theta = \theta_*\right\} \nonumber \\
    &\geq 1 - k
    \label{eq:covbound}
\end{align}
where the first equality follows from the definition of a $k$ support
set~\eqref{eq:ss}, whereas the inequality follows from the universal
bound~\eqref{eq:ubound}. This shows that a $k$ support set with $k < 1$ is also
a valid $(1 - k)100\%$ confidence sets, even under sequential analyses with
optional stopping. Of course, the coverage bound rests on the assumption that
the data model is correctly specified and a misspecified data model will result
in incorrect coverage. Furthermore, the bound is based on simple null
hypotheses, but it can also be shown to hold for composite null hypotheses when
special types of priors are assigned to the nuisance parameters
\citep{Hendriksen2021}.

For the case of a univariate parameter $\theta$ as considered earlier,
construction of $(1 - k)\%$ approximate confidence interval via the the normal
prior support interval from~\eqref{eq:si} corresponds to the proposal by
\citet{Pace2020}. These authors studied this particular case in detail and gave
also frequentist motivations for the prior distributions interpreting them as
weighting functions. Moreover, they found that the method is also applicable to
parameter estimates from marginal, conditional, and profile likelihoods, and
that the coverage of the intervals is controlled even under slight model
misspecifications. We refer to \citet{Pace2020} for further details.

A $k < 1$ support interval will usually be wider than a standard $(1 - k)\%$
confidence interval. On the other hand, a $k < 1$ support interval has at least
$(1 - k)\%$ coverage, even under optional stopping (at least for point null
hypotheses as is the case here), which is not satisfied by a standard
$(1 - k)\%$ confidence interval. Due to their property of valid coverage based
on \emph{arbitrary number} of looks at the data, $k < 1$ support interval will
also typically be wider than $(1 - k)\%$ confidence intervals adjusted via group
sequential or adaptive trial methodology which are more fine-tuned to specific
interim analysis strategies \citep{Wassmer2016}. These strategies are, however,
typically more restrictive and computationally involved compared to the flexible
and easily computable $k < 1$ support intervals which we present here.

It must be noted that the coverage bound~\eqref{eq:covbound} only holds for
support intervals but not for minimum support intervals. This is because the
minimum support intervals are derived based on priors that depend on the data,
which violates the assumption of the universal bound. Minimum support intervals
are thus only useful for giving confidence intervals an evidential
interpretation, but a $k$ minimum support interval with $k < 1$, itself does not
provide $(1 - k)\%$ coverage under optional stopping.


\section{Discussion}
Misinterpretations and misconceptions of confidence intervals are common
\citep{Hoekstra2014, Greenland2016}. We showed how confidence intervals can be
reinterpreted as minimum support intervals which have an intuitive
interpretation in terms of the minimum evidence that the data provide for the
included parameter values. We also obtained easy to use formulas for different
types of support intervals for an unknown parameter based on an estimate and
standard error thereof. Table~\ref{tab:summary} summarizes our results.

%% place at appropriate place in preprint/journal
\begin{table*}[!htb]
    \centering
    \caption{Summary of confidence intervals (CI), support intervals (SI), and
      minimum support intervals (minSI) for an unknown parameter $\theta$ based
      on a parameter estimate $\that$ with standard error $\sigma$. All
      intervals are of the form $\that \pm \sigma \times \mbox{M}$. To transform
      an interval from type A to type B, first subtract $\that$ from the
      boundaries of the interval, multiply by the ratio of the standard error
      multipliers $\mbox{M}_{\text{B}}/\mbox{M}_{\text{A}}$, and add again
      $\that$ to the boundaries of the interval. The standard error multipliers
      $\mbox{M}$ depend on either the confidence level $(1 - \alpha)$ or the
      support level $k$. For the support intervals, the standard error
      multipliers M additionally depend on the parameters of the prior for
      $\theta$ under the alternative hypothesis: mean $\mu_\theta$ and variance
      $\sigma_\theta^{2}$ for the normal prior, variance $\sigma_\theta^{2}$ for
      the local normal prior, and scale $\sigma_\theta$ for the nonlocal normal
      moment prior. The quantile function of the standard normal distribution is
      denoted by $\Phi^{-1}(\cdot)$, $\lw{0}(\cdot)$ denotes the principal
      branch of the Lambert W function \citep{Corless1996}, and $\lw{-1}(\cdot)$
      denotes the branch that satisfies $\mbox{W}(y) \leq 1$ for
      $y \in [-1/e, 0)$. The respective (minimum) support intervals only exist
      for support levels $k$ for which the standard error multiplier is
      real-valued, \ie{} the respective term in the square root needs to be
      non-negative and/or the argument for $\lw{-1}(\cdot)$ needs to be in
      $[-1/e, 0)$. All interval types can be computed with the R package
      \texttt{ciCalibrate} (Appendix~\ref{app:Rpkg}).}
    \label{tab:summary}
    \begin{tabular}{lll}
    \toprule
    Interval type & %(Implied) prior for $\theta$ &
    Standard error multiplier M \\
    \midrule
    $(1 - \alpha)100\%$ CI %& $\theta \sim \Nor(\mu_\theta, \infty)$
    & $\Phi^{-1}(1 - \alpha/2)$ \vspace{0.2em}\\
    % $(1 - \alpha)\%$ Credible & \theta \sim \Nor(\mu_\theta, \sigma_\theta^2) & \\
    $k$ $\mbox{SI}$ (normal prior) & %$\theta \sim \Nor(\mu_\theta, \sigma_\theta^2), ~ m \neq \theta_0$ &
    $\surd \{\log(1 + \sigma_\theta^2/\sigma^2) + (\that - \mu_\theta)^2/(\sigma^2 + \sigma_\theta^2) -
    2\log k\}$ \vspace{0.2em}\\
    $k$ $\mbox{SI}$ (local normal prior) & %$\theta \sim \Nor(\theta_0, \sigma_\theta^2)$ &
    $\surd [\{\log(1 + \sigma_\theta^2/\sigma^{2}) - 2\log k \}
    (1 + \sigma^{2}/\sigma_\theta^2)]$ \vspace{0.2em}\\
    $k$ $\mbox{SI}$ (nonlocal normal moment prior) &
    $\surd ([2 \lw{0}\{(1 + \sigma_\theta^2/\sigma^2)^{3/2}/(2 k e^{-1/2})\} - 1]\{1 + \sigma^2/\sigma_\theta^2\})$ \vspace{0.2em}\\
    $k$ $\mbox{minSI}$ (all priors) & $\surd (-2\log k)$ \vspace{0.2em}\\
    $k$ $\mbox{minSI}$ (local normal priors) & $\surd \{-\lw{-1}(-k^2/ e)\}$ \vspace{0.2em}\\
    $k$ $\mbox{minSI}$ ($-e\, p\, \log p$) &
    $\Phi^{-1}[1 - \exp\{\lw{-1}(-k/e)\}/2]$ \vspace{0.2em}\\
    \bottomrule
    \end{tabular}
  \end{table*}

Which type of support interval should data analysts use in practice? We believe
that the support interval based on a normal prior distribution is the most
intuitive for encoding external knowledge. This type should therefore be
preferably used whenever external knowledge is available. At the same time, the
support interval based on a local normal prior with unit-information variance
\citep{Kass1995b} seems to be a reasonable ``default'' choice in cases where no
external knowledge is available. Finally, we believe that minimum support
intervals are mostly useful for giving confidence intervals an evidential
interpretation due to the one-to-one mapping between the two.

It is also not clear which support level $k$ should be used for computing
support intervals. If space permits, we recommend to visualize the Bayes factor
as a function of the null value as in Figure~\ref{fig:example}. This approach
provides readers with a more gradual assessment of support, and any desired $k$
support interval can be read off from it. If there are space constraints, a
compromise is to report support intervals for different levels (\eg{}
$k \in \{1/10, 1, 10\}$) or to present a forest plot with ``telescope'' style
support intervals with ascending support levels stacked on top of each other, as
in Figure~\ref{fig:comparison}. We are hesitant to recommend a ``default''
support level because any classification of support is arbitrary, just like the
95\% confidence level convention. We believe that $k = 1$ is perhaps the least
arbitrary default level, as it represents the tipping point at which the
included parameter values begin to receive support from the data (although not
necessarily strong support).

Other approaches for reinterpreting confidence intervals have been proposed. For
instance, \citet{Rafi2020} propose to rename confidence intervals to
``compatibility'' intervals and give their confidence level an information
theoretic interpretation. For example, a 95\% confidence interval contains
parameter values with at most 4.3 bits refutational ``surprisal''. We believe
that this is a step in the right direction; however, we also believe that the
term ``compatibility'' may not be entirely appropriate as absence of surprisal
does not establish compatibility. To do so, we think that an evidential
framework is necessary and that the specification of a prior for the parameter
under the alternative hypothesis is unavoidable. Compatibility intervals are in
this sense similar to minimum support intervals; without a specified prior under
the alternative hypothesis only the maximum surprisal/evidence \emph{against}
the included parameter values can be quantified.

We also showed how the coverage of $k$ support intervals with $k < 1$ is bounded
by $(1 - k)\%$, which holds even under sequential analyses with optional
stopping. For instance, a $k = 1/20$ support interval has valid $95\%$ coverage.
Of course, such error rate guarantees rest on the assumption that the data model
has been correctly specified, which in most real world applications will be
violated to some extent. We do not see this as a problem for the evidential
interpretation of support intervals, which is usually of more concern to data
analysts. Evidential inference does not rely on a statistical model being
``true'' in some abstract sense. Bayes factors and support intervals simply
quantify the relative predictive performance that the combination of data model
and parameter distribution yield on out-of-sample data \citep{Kass1995,
  OHagan2004, Gneiting2007a}. Such ``descriptive inferential statistics'' are
especially important for the analysis of convenience data samples which
typically violate assumptions of the underlying statistical model
% , so that overconfident error-rate statements are not appropriate leading to
% overconfident error rate statements from $p$-values and confidence intervals
% are overconfident
\citep{Amrhein2019, Shafer2021}. In fact, even one of the best known proponents
of $p$-values --R.A. Fisher-- noted ``For all purposes, and more particularly
for the \emph{communication} of the relevant evidence supplied by a body of
data, the values of the Mathematical Likelihood are better fitted to analyse,
summarize, and communicate statistical evidence of types too weak to supply true
probability statements'' \citet[p. 70]{Fisher1956} clearly recognizing the
importance of inferential tools based on relative likelihood for making sense
out of data.



\section*{Software and data}
The point estimate and 95\% confidence interval of the adjusted log hazard ratio
were extracted from the abstract of \citet{RECOVERY2021}. All analyses were
conducted in the R programming language version
\Sexpr{paste(version$major, version$minor, sep = ".")}. Code and data for
reproducing the results in this manuscript are available at
\url{https://github.com/SamCH93/ECoCI}. A snapshot of the GitHub repository at
the time of writing this article is archived at
\url{https://doi.org/10.5281/zenodo.6723249}. An R package for calibration of
confidence intervals to (minimum) support intervals is available at
\url{https://github.com/SamCH93/ciCalibrate}, see Appendix~\ref{app:Rpkg} for an
illustration.

\section*{Acknowledgments}
We thank Leonhard Held for helpful comments on an earlier version of the
manuscript. We thank Michael Lindon for interesting discussions and letting us
know about his work on the connection between support and confidence sets. We
thank Glenn Shafer for attending us about R.A. Fisher's work on relative
likelihood. Our acknowledgement of these individuals does not imply their
endorsement of this article. This work was supported in part by an NWO Vici
grant (016.Vici.170.083) to EJW, and a Swiss National Science Foundation
mobility grant (part of 189295) to SP.


%% Appendix
%% -----------------------------------------------------------------------------
\begin{appendices}
% \section{The universal bound}
% \label{app:universalBound}
% We briefly review the universal bound \citep[Ch. 1.4]{Royall1997} for Bayes
% factors. Let $0 < k < 1$ and $\mathcal{X}_k = \left\{x :
%   \BF_{01}(x) %= \frac{f(x\given\h{0})}{f(x\given\h{1})}
%   \leq k \right\}$ so that the probability for observing misleading evidence
% $\BF_{01}(x) \leq k$ when $\h{0}$ is true is
% \begin{align*}
%     \P\left(\BF_{01}(X) \leq k \given \h{0}\right)
%     &= \int_{\mathcal{X}_k} f(x\given \h{0}) \,\text{d}x \\
%     &\leq \int_{\mathcal{X}_k} k\, f(x\given \h{1}) \,\text{d}x \\
%     &\leq k.
% \end{align*}
% The first inequality follows from the fact that
% $f(x \given \h{0}) \leq k \,f(x \given \h{1})$ for all $x \in \mathcal{X}_k$
% by definition of $\mathcal{X}_k$, while the second inequality follows from the
% fact that the integral of $f(x\given \h{1})$ over $\mathcal{X}_k$ can at most
% be one. A generalization of this result to sequential analysis of data was
% first established by \citet{Robbins1970}. Appendix A1 in \citet{Pace2020}
% gives his original proof in modern notation.

\section{The ciCalibrate package}
\label{app:Rpkg}
We provide an R implementation of the support intervals and underlying Bayes
factor functions from Table~\ref{tab:summary}. The package is available at
\url{https://github.com/SamCH93/ciCalibrate}. The following code snippet
illustrates the computation and plotting of support interval and Bayes factor
function.

<< "code-snippet-ciCalibrate", echo = TRUE, fig.height = ifelse(type == "journal", 4, 3.6), size = codeFont >>=
## 95% CI from RECOVERY trial
logHRci <- c(-0.29, -0.07)
## compute a support interval with level k = 10
library("ciCalibrate")
si10 <- ciCalibrate(ci = logHRci, ciLevel = 0.95,
                    siLevel = 10,
                    method = "SI-normal",
                    priorMean = 0, priorSD = 2)
si10
## plot Bayes factor function with support interval
plot(si10)
@

\end{appendices}
